{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2e7c891-16fd-4b61-8590-eebc08adcb3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Notebook: 01_Data_Ingestion_Medallion.py\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# --- 1. CONFIGURAÃ‡ÃƒO DO UNITY CATALOG E ARQUIVOS ---\n",
    "UC_CATALOG = \"dev_catalogue\"\n",
    "UC_SCHEMA = \"staging_schema\"\n",
    "BRONZE_TABLE = f\"{UC_CATALOG}.{UC_SCHEMA}.bronze_insurance_costs\"\n",
    "\n",
    "# CORREÃ‡ÃƒO: Usando o caminho do Volume do Unity Catalog\n",
    "# ASSUMINDO que o arquivo 'insurance.csv' estÃ¡ dentro deste Volume.\n",
    "CSV_FILE_PATH = \"/Volumes/dev_catalogue/staging_schema/raw_data_volume/insurance.csv\" \n",
    "\n",
    "\n",
    "# Cria o schema se nÃ£o existir (garantia)\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {UC_CATALOG}.{UC_SCHEMA}\")\n",
    "print(f\"Schema {UC_CATALOG}.{UC_SCHEMA} verificado.\")\n",
    "\n",
    "\n",
    "# --- 2. LEITURA DO ARQUIVO BRUTO A PARTIR DO VOLUME ---\n",
    "print(f\"Lendo dados brutos de: {CSV_FILE_PATH}\")\n",
    "\n",
    "# Leitura do CSV: infere o esquema e considera o cabeÃ§alho\n",
    "df_raw = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(CSV_FILE_PATH)\n",
    ")\n",
    "\n",
    "print(\"\\nPrimeiras 5 linhas do DataFrame BRONZE lido do Volume:\")\n",
    "df_raw.show(5)\n",
    "print(f\"Esquema inferido: {df_raw.printSchema()}\")\n",
    "\n",
    "# --- 3. SALVAMENTO NA CAMADA BRONZE (DELTA LAKE) ---\n",
    "# Reescrevemos a tabela Bronze. Como ela jÃ¡ existe, o modo 'overwrite' a atualiza.\n",
    "df_raw.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(BRONZE_TABLE)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"âœ… Camada BRONZE atualizada com sucesso na Tabela Delta: {BRONZE_TABLE}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7870401-1f3b-4a73-ac8a-3efa0664eb88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CÃ©lula 2: CriaÃ§Ã£o da Camada SILVER (Limpeza e Chaves)\n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id, col, regexp_replace\n",
    "\n",
    "# --- 1. CONFIGURAÃ‡ÃƒO DE CAMINHOS ---\n",
    "BRONZE_TABLE = \"dev_catalogue.staging_schema.bronze_insurance_costs\"\n",
    "SILVER_TABLE = \"dev_catalogue.staging_schema.silver_insurance_features\"\n",
    "\n",
    "# Leitura da Tabela Bronze\n",
    "df_silver = spark.read.table(BRONZE_TABLE)\n",
    "\n",
    "print(f\"Lido {df_silver.count()} registros da Camada BRONZE.\")\n",
    "\n",
    "# --- 2. PADRONIZAÃ‡ÃƒO E LIMPEZA BÃSICA ---\n",
    "# 2.1 Padronizar nomes de colunas (Snake_Case)\n",
    "df_silver = df_silver.select(\n",
    "    col(\"age\"),\n",
    "    col(\"sex\"),\n",
    "    col(\"bmi\"),\n",
    "    col(\"children\"),\n",
    "    col(\"smoker\"),\n",
    "    # Padronizar a coluna 'region'\n",
    "    regexp_replace(col(\"region\"), \"-\", \"_\").alias(\"region\"), \n",
    "    # Renomear 'charges' para 'insurance_cost' (melhor para BI)\n",
    "    col(\"charges\").alias(\"insurance_cost\")\n",
    ")\n",
    "\n",
    "# 2.2 Tratamento de nulos (NÃ£o esperado neste dataset, mas boa prÃ¡tica)\n",
    "# Exemplo: remover linhas onde a coluna principal 'insurance_cost' for nula.\n",
    "df_silver = df_silver.filter(col(\"insurance_cost\").isNotNull())\n",
    "\n",
    "\n",
    "# --- 3. CRIAÃ‡ÃƒO DE CHAVES SUBSTITUTAS (SURROGATE KEYS) ---\n",
    "# O 'monotonically_increasing_id' Ã© usado para gerar chaves exclusivas.\n",
    "# Essas chaves serÃ£o as Pks das DimensÃµes e Fks da Fato.\n",
    "\n",
    "df_silver = df_silver.withColumn(\n",
    "    \"SK_ID\", # Chave PrimÃ¡ria Substituta\n",
    "    monotonically_increasing_id()\n",
    ")\n",
    "\n",
    "# Criar chaves substitutas para as dimensÃµes:\n",
    "# Nota: Usamos a mesma SK_ID para todas as DimensÃµes que se relacionarÃ£o\n",
    "# diretamente com a Tabela Fato no Esquema Estrela.\n",
    "\n",
    "df_silver = df_silver.withColumn(\n",
    "    \"SK_LOCATION\",\n",
    "    monotonically_increasing_id()\n",
    ").withColumn(\n",
    "    \"SK_HABIT\",\n",
    "    monotonically_increasing_id()\n",
    ").withColumn(\n",
    "    \"SK_TIME\",\n",
    "    F.lit(20250101).cast(\"int\") # Chave fixa temporÃ¡ria, pois a data nÃ£o estÃ¡ no dataset\n",
    ")\n",
    "\n",
    "\n",
    "# --- 4. SELEÃ‡ÃƒO FINAL DA CAMADA SILVER ---\n",
    "# Selecionamos todas as colunas essenciais, incluindo as novas chaves\n",
    "df_silver_final = df_silver.select(\n",
    "    \"SK_ID\",\n",
    "    \"SK_LOCATION\",\n",
    "    \"SK_HABIT\",\n",
    "    \"SK_TIME\",\n",
    "    \n",
    "    # Atributos (Features)\n",
    "    \"age\",\n",
    "    \"sex\",\n",
    "    \"bmi\",\n",
    "    \"children\",\n",
    "    \"smoker\",\n",
    "    \"region\",\n",
    "    \n",
    "    # MÃ©trica (Label)\n",
    "    \"insurance_cost\"\n",
    ")\n",
    "\n",
    "# --- 5. SALVAR NA CAMADA SILVER ---\n",
    "df_silver_final.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(SILVER_TABLE)\n",
    "\n",
    "print(\"\\nPrimeiras 5 linhas da Camada SILVER (Com Chaves SK):\")\n",
    "df_silver_final.show(5)\n",
    "print(f\"Total de registros na SILVER: {df_silver_final.count()}\")\n",
    "print(f\"âœ… Camada SILVER (Limpa e com SKs) salva com sucesso em: {SILVER_TABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f183c74-40c8-443f-8ab9-d33e9ad38a7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CÃ©lula 3: CriaÃ§Ã£o da Camada GOLD (Modelagem Dimensional: Estrela e Floco de Neve)\n",
    "\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "# --- 1. CONFIGURAÃ‡ÃƒO E LEITURA DA SILVER ---\n",
    "SILVER_TABLE = \"dev_catalogue.staging_schema.silver_insurance_features\"\n",
    "df_silver = spark.read.table(SILVER_TABLE)\n",
    "\n",
    "# --- 2. CRIAÃ‡ÃƒO DA TABELA FATO (Fact_Insurance_Cost) ---\n",
    "FACT_TABLE = \"dev_catalogue.staging_schema.gold_fact_insurance_cost\"\n",
    "\n",
    "# A Tabela Fato contÃ©m mÃ©tricas e todas as FKs.\n",
    "df_fact = df_silver.select(\n",
    "    col(\"SK_ID\").alias(\"Insurance_SK_ID\"), # PK/FK principal\n",
    "    col(\"SK_LOCATION\"),\n",
    "    col(\"SK_HABIT\"),\n",
    "    col(\"SK_TIME\"),\n",
    "    col(\"age\"),\n",
    "    col(\"bmi\"),\n",
    "    col(\"children\"),\n",
    "    col(\"insurance_cost\")\n",
    ")\n",
    "\n",
    "# Salvar a Tabela Fato\n",
    "df_fact.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(FACT_TABLE)\n",
    "\n",
    "print(f\"âœ… Tabela FATO CENTRAL criada em: {FACT_TABLE}\")\n",
    "print(\"---\")\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# ðŸŒŸ ESQUEMA ESTRELA (STAR SCHEMA) - Otimizado para Queries Simples (Databricks SQL)\n",
    "# ==========================================================\n",
    "\n",
    "# 2.1 DIMENSÃƒO LOCALIZAÃ‡ÃƒO (Estrela - Desnormalizada)\n",
    "DIM_LOCATION_STAR_TABLE = \"dev_catalogue.staging_schema.gold_dim_location_star\"\n",
    "\n",
    "df_dim_location_star = df_silver.select(\n",
    "    col(\"SK_LOCATION\"),\n",
    "    col(\"region\").alias(\"Region_Name\") # Atributos agrupados\n",
    ").distinct()\n",
    "\n",
    "df_dim_location_star.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(DIM_LOCATION_STAR_TABLE)\n",
    "\n",
    "print(f\"âœ… DimensÃ£o Estrela (LOCATION) criada em: {DIM_LOCATION_STAR_TABLE}\")\n",
    "\n",
    "\n",
    "# 2.2 DIMENSÃƒO HÃBITOS (Estrela - Desnormalizada)\n",
    "DIM_HABIT_STAR_TABLE = \"dev_catalogue.staging_schema.gold_dim_habit_star\"\n",
    "\n",
    "df_dim_habit_star = df_silver.select(\n",
    "    col(\"SK_HABIT\"),\n",
    "    col(\"sex\").alias(\"Gender\"),\n",
    "    col(\"smoker\").alias(\"Smoker_Status\") # Atributos agrupados\n",
    ").distinct()\n",
    "\n",
    "df_dim_habit_star.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(DIM_HABIT_STAR_TABLE)\n",
    "\n",
    "print(f\"âœ… DimensÃ£o Estrela (HABIT) criada em: {DIM_HABIT_STAR_TABLE}\")\n",
    "print(\"---\")\n",
    "\n",
    "# ==========================================================\n",
    "# â„ï¸ ESQUEMA FLOCO DE NEVE (SNOWFLAKE SCHEMA) - Otimizado para NormalizaÃ§Ã£o (Power BI)\n",
    "# ==========================================================\n",
    "\n",
    "# 3.1 DIMENSÃƒO HÃBITOS (Floco de Neve - Normalizada)\n",
    "DIM_HABIT_SNOW_TABLE = \"dev_catalogue.staging_schema.gold_dim_habit_snow\"\n",
    "DIM_SMOKER_STATUS_TABLE = \"dev_catalogue.staging_schema.gold_dim_smoker_status\"\n",
    "\n",
    "# Tabela 3.1.a: Sub-DimensÃ£o (Status do Fumante)\n",
    "df_dim_smoker = df_silver.select(\n",
    "    col(\"SK_HABIT\").alias(\"Smoker_SK\"),\n",
    "    col(\"smoker\").alias(\"Smoker_Status\")\n",
    ").distinct()\n",
    "\n",
    "df_dim_smoker.write.format(\"delta\").mode(\"overwrite\").saveAsTable(DIM_SMOKER_STATUS_TABLE)\n",
    "\n",
    "# Tabela 3.1.b: DimensÃ£o Principal (Habit)\n",
    "# Esta tabela se conecta Ã  FATOS e contÃ©m apenas a FK para a Sub-DimensÃ£o\n",
    "df_dim_habit_snow = df_silver.select(\n",
    "    col(\"SK_HABIT\"),\n",
    "    col(\"sex\").alias(\"Gender\"),\n",
    "    col(\"SK_HABIT\").alias(\"Smoker_SK\") # Usamos a SK_HABIT como FK para Smoker_Status para simplificar\n",
    ").distinct()\n",
    "\n",
    "df_dim_habit_snow.write.format(\"delta\").mode(\"overwrite\").saveAsTable(DIM_HABIT_SNOW_TABLE)\n",
    "\n",
    "print(f\"âœ… DimensÃ£o Floco de Neve (HABIT) e Sub-DimensÃ£o (SMOKER) criadas.\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"FLUXO DE DADOS COMPLETO: BRONZE -> SILVER -> GOLD (STAR/SNOWFLAKE)\")\n",
    "\n",
    "# Exemplo de como o Snowflake se parece:\n",
    "# Fact_Insurance_Cost -> Dim_Habit_Snow -> Dim_Smoker_Status"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Data_Ingestion_Medallion.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
